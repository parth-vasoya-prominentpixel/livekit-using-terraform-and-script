name: üé• LiveKit Deployment Pipeline

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy
      environment:
        description: 'Environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - uat
          - prod
      skip_prerequisites:
        description: 'Skip Prerequisites Check'
        required: false
        default: false
        type: boolean
      skip_terraform_plan:
        description: 'Skip Terraform Plan'
        required: false
        default: false
        type: boolean
      skip_terraform_apply:
        description: 'Skip Terraform Apply'
        required: false
        default: false
        type: boolean
      skip_eks_access:
        description: 'Skip EKS Access Policy & ACM Certificate Configuration'
        required: false
        default: false
        type: boolean
      skip_load_balancer:
        description: 'Skip Load Balancer Controller Setup'
        required: false
        default: false
        type: boolean
      skip_livekit:
        description: 'Skip LiveKit Deployment'
        required: false
        default: false
        type: boolean
      domain_name:
        description: 'Domain name for SSL certificate (e.g., livekit.example.com)'
        required: true
        default: 'livekit.example.com'
        type: string
      hosted_zone_id:
        description: 'Route53 Hosted Zone ID for DNS validation'
        default: 'Z023244434BR682QISWOZ'
        required: true
        type: string

env:
  AWS_REGION: us-east-1

permissions:
  id-token: write
  contents: read

jobs:
  # Step 1: Prerequisites (Optional)
  prerequisites:
    name: üîß Step 1 - Prerequisites
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'deploy' && !inputs.skip_prerequisites }}
    environment: livekit-poc-${{ inputs.environment }}-prerequisites
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Prerequisites-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Setup Terraform
          wget -q https://releases.hashicorp.com/terraform/1.10.3/terraform_1.10.3_linux_amd64.zip
          unzip -q terraform_1.10.3_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          
          # Setup kubectl
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Setup Helm
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Setup eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v0.191.0/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/

      - name: Run Prerequisites Script
        working-directory: scripts
        run: |
          chmod +x 00-prerequisites.sh
          ./00-prerequisites.sh

  # Step 2: Terraform Plan (Optional)
  terraform-plan:
    name: üìã Step 2 - Terraform Plan
    runs-on: ubuntu-latest
    needs: [prerequisites]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_terraform_plan && (needs.prerequisites.result == 'success' || inputs.skip_prerequisites) }}
    environment: livekit-poc-${{ inputs.environment }}-terraform-plan
    outputs:
      plan-output: ${{ steps.plan.outputs.plan_output }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Terraform-Plan-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform-version: 1.10.3

      - name: Terraform Plan
        id: plan
        working-directory: resources
        run: |
          echo "üìã Running Terraform Plan..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          terraform plan \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -out=tfplan
          
          echo "‚úÖ Terraform plan completed successfully!"

  # Step 3: Terraform Apply (Optional)
  terraform-apply:
    name: üöÄ Step 3 - Terraform Apply
    runs-on: ubuntu-latest
    needs: [terraform-plan]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_terraform_apply && (needs.terraform-plan.result == 'success' || inputs.skip_terraform_plan) }}
    environment: livekit-poc-${{ inputs.environment }}-terraform-apply
    outputs:
      cluster-name: ${{ steps.apply.outputs.cluster_name }}
      redis-endpoint: ${{ steps.apply.outputs.redis_endpoint }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Terraform-Apply-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform-version: 1.10.3

      - name: Terraform Apply
        id: apply
        working-directory: resources
        run: |
          echo "üöÄ Applying Terraform configuration..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          terraform plan \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -out=tfplan
          
          terraform apply tfplan
          
          # Get outputs
          CLUSTER_NAME=$(terraform output -raw cluster_name)
          REDIS_ENDPOINT=$(terraform output -raw redis_cluster_endpoint)
          
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "redis_endpoint=$REDIS_ENDPOINT" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Infrastructure deployed successfully!"
          echo "üìä Cluster: $CLUSTER_NAME"
          echo "üìä Redis: $REDIS_ENDPOINT"

  # Step 4: Configure EKS Access Policies & ACM Certificate
  eks-access-and-acm:
    name: üîê Step 4 - Configure EKS Access & ACM Certificate
    runs-on: ubuntu-latest
    needs: [terraform-apply]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_eks_access && (needs.terraform-apply.result == 'success' || inputs.skip_terraform_apply) }}
    environment: livekit-poc-${{ inputs.environment }}-eks-access-acm
    
    steps:
      - name: üìã Manual Approval - Configure EKS Access & ACM Certificate
        run: |
          echo "ü§î Manual approval required to configure EKS access policies and ACM certificate"
          echo "üìã This will:"
          echo "   - Grant AmazonEKSAdminPolicy to pipeline role"
          echo "   - Grant AmazonEKSClusterAdminPolicy to pipeline role"
          echo "   - Update kubeconfig for cluster access"
          echo "   - Request ACM SSL certificate for domain"
          echo "   - Create DNS validation records in Route53"
          echo "   - Wait for certificate issuance"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "‚è±Ô∏è Time: ~5-8 minutes"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-EKS-Access-ACM-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl for cluster verification
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Verify installation
          echo "‚úÖ Tool versions:"
          kubectl version --client

      - name: Configure EKS Access Policies & ACM Certificate
        working-directory: scripts
        run: |
          chmod +x 01-setup-eks-access-and-acm.sh
          
          # Determine cluster name
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
          fi
          
          echo "üìã Configuration for EKS Access Policy & ACM Certificate:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Pipeline Role: ${{ secrets.AWS_OIDC_ROLE_ARN }}"
          echo "üìã Domain: ${{ inputs.domain_name }}"
          echo "üìã Hosted Zone ID: ${{ inputs.hosted_zone_id }}"
          echo ""
          
          # Enable full logging
          set -x
          
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          PIPELINE_ROLE_ARN="${{ secrets.AWS_OIDC_ROLE_ARN }}" \
          DOMAIN_NAME="${{ inputs.domain_name }}" \
          HOSTED_ZONE_ID="${{ inputs.hosted_zone_id }}" \
          CERT_REGION="${{ env.AWS_REGION }}" \
          ./01-setup-eks-access-and-acm.sh 2>&1 | tee /tmp/eks-access-acm.log
          
          echo ""
          echo "üìã EKS Access & ACM Certificate Log Summary:"
          echo "‚úÖ EKS access policy and ACM certificate configuration completed"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 5: Setup AWS Load Balancer Controller
  setup-load-balancer-controller:
    name: üîß Step 5 - Setup AWS Load Balancer Controller
    runs-on: ubuntu-latest
    needs: [eks-access-and-acm]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_load_balancer && (needs.eks-access-and-acm.result == 'success' || inputs.skip_eks_access) }}
    environment: livekit-poc-${{ inputs.environment }}-setup-load-balancer
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LoadBalancer-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Install Helm
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v0.191.0/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/
          
          # Install jq and curl
          sudo apt-get update && sudo apt-get install -y jq curl
          
          # Verify installations
          echo "‚úÖ Tool versions:"
          kubectl version --client
          helm version
          eksctl version
          jq --version

      - name: Setup AWS Load Balancer Controller
        working-directory: scripts
        run: |
          chmod +x 02-setup-load-balancer-controller.sh
          
          # Determine cluster name
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
          fi
          
          echo "üìã Configuration for Load Balancer Controller Setup:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo ""
          
          # Enable full logging and continuous output
          set -x  # Enable command tracing
          
          # Run the Load Balancer Controller setup with full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          ./02-setup-load-balancer-controller.sh 2>&1 | tee /tmp/lbc-setup.log
          
          # Show final status
          echo ""
          echo "üîç Final Status Check:"
          kubectl get deployment aws-load-balancer-controller -n kube-system || echo "Deployment not found"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "No pods found"
          kubectl get service aws-load-balancer-webhook-service -n kube-system || echo "Webhook service not found"
          
          echo ""
          echo "üìã Setup Log Summary:"
          echo "‚úÖ Load Balancer Controller setup completed"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 6: Deploy LiveKit
  deploy-livekit:
    name: üé• Step 6 - Deploy LiveKit
    runs-on: ubuntu-latest
    needs: [setup-load-balancer-controller]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_livekit && (needs.setup-load-balancer-controller.result == 'success' || inputs.skip_load_balancer) }}
    environment: livekit-poc-${{ inputs.environment }}-deploy-livekit
    
    steps:
      - name: üìã Manual Approval - Deploy LiveKit
        run: |
          echo "ü§î Manual approval required to deploy LiveKit"
          echo "üìã This will deploy:"
          echo "   - LiveKit Server using official Helm chart"
          echo "   - ALB Ingress with SSL certificate"
          echo "   - TURN server for WebRTC connectivity"
          echo "   - Metrics and Prometheus monitoring"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "‚è±Ô∏è Time: ~5-10 minutes"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Deploy-LiveKit-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install jq for JSON processing
          sudo apt-get update && sudo apt-get install -y jq
          
          # Verify installations
          echo "‚úÖ Tool versions:"
          kubectl version --client
          helm version
          jq --version

      - name: Deploy LiveKit
        working-directory: scripts
        run: |
          chmod +x 03-deploy-livekit.sh
          
          # Determine cluster name and Redis endpoint
          # Use hardcoded values since infrastructure is already deployed
          CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          REDIS_ENDPOINT="lp-ec-redis-use1-dev-redis.x4ncn3.ng.0001.use1.cache.amazonaws.com:6379"
          
          echo "üìã Configuration for LiveKit Deployment:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Redis: $REDIS_ENDPOINT"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo ""
          
          # Enable full logging and continuous output
          set -x  # Enable command tracing
          
          # Deploy LiveKit with configuration and full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          REDIS_ENDPOINT="$REDIS_ENDPOINT" \
          ENVIRONMENT="${{ inputs.environment }}" \
          ./03-deploy-livekit.sh 2>&1 | tee /tmp/livekit-deployment.log
          
          # Show final status
          echo ""
          echo "üîç Final Status Check:"
          kubectl get deployment -n livekit || echo "No deployments found"
          kubectl get pods -n livekit || echo "No pods found"
          kubectl get ingress -n livekit || echo "No ingress found"
          kubectl get services -n livekit || echo "No services found"
          
          echo ""
          echo "üìã Deployment Log Summary:"
          echo "‚úÖ LiveKit deployment completed"
          echo "üìÑ Full logs available in pipeline output above"

  # DESTROY: Remove everything (Optional)
  destroy:
    name: üóëÔ∏è Destroy LiveKit Infrastructure
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'destroy' }}
    environment: livekit-poc-${{ inputs.environment }}-destroy
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-Destroy-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          wget -q https://releases.hashicorp.com/terraform/1.10.3/terraform_1.10.3_linux_amd64.zip
          unzip -q terraform_1.10.3_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/

      - name: üóëÔ∏è Destroy All Infrastructure
        working-directory: resources
        run: |
          echo "üóëÔ∏è Destroying all infrastructure with Terraform..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          # Run destroy with retries
          MAX_ATTEMPTS=3
          ATTEMPT=1
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "üîÑ Destroy attempt $ATTEMPT/$MAX_ATTEMPTS..."
            
            if terraform destroy \
              -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
              -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
              -auto-approve; then
              echo "‚úÖ All infrastructure destroyed!"
              exit 0
            else
              echo "‚ö†Ô∏è Destroy attempt $ATTEMPT failed"
              
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "‚ùå Destroy failed after $MAX_ATTEMPTS attempts"
                exit 1
              fi
              
              echo "‚è≥ Waiting 10 seconds before retry..."
              sleep 10
              ATTEMPT=$((ATTEMPT + 1))
            fi
          done