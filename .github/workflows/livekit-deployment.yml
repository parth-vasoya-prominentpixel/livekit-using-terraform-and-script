name: üé• LiveKit Deployment Pipeline

# Available Skip Options:
# - skip_prerequisites: Skip tools installation and validation
# - skip_terraform_plan: Skip Terraform planning phase  
# - skip_terraform_apply: Skip infrastructure deployment
# - skip_eks_access: Skip EKS access policies & ACM certificate setup
# - skip_load_balancer: Skip AWS Load Balancer Controller installation
# - skip_livekit: Skip LiveKit application deployment
# - skip_dns_setup: Skip DNS records creation for domains
# - skip_sip_server: Skip SIP Server deployment
# - skip_kamailio: Skip Kamailio SIP Load Balancer deployment

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy
      environment:
        description: 'Environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - uat
          - prod
      skip_prerequisites:
        description: 'Skip Prerequisites Check'
        required: false
        default: false
        type: boolean
      skip_terraform_plan:
        description: 'Skip Terraform Plan'
        required: false
        default: false
        type: boolean
      skip_terraform_apply:
        description: 'Skip Terraform Apply'
        required: false
        default: false
        type: boolean
      skip_eks_access:
        description: 'Skip EKS Access Policy & ACM Certificate Configuration'
        required: false
        default: false
        type: boolean
      skip_load_balancer:
        description: 'Skip Load Balancer Controller Setup'
        required: false
        default: false
        type: boolean
      skip_livekit:
        description: 'Skip LiveKit Deployment'
        required: false
        default: false
        type: boolean
      skip_dns_setup:
        description: 'Skip DNS Records Setup'
        required: false
        default: false
        type: boolean
      skip_sip_server:
        description: 'Skip SIP Server Deployment'
        required: false
        default: false
        type: boolean
      skip_kamailio:
        description: 'Skip Kamailio SIP Load Balancer Deployment'
        required: false
        default: false
        type: boolean
      domain_name:
        description: 'Domain name for SSL certificate (e.g., livekit.example.com)'
        required: true
        default: 'livekit-eks-tf.digi-telephony.com'
        type: string
      hosted_zone_id:
        description: 'Route53 Hosted Zone ID for DNS validation'
        default: 'Z023244434BR682QISWOZ'
        required: true
        type: string
      manual_alb_endpoint:
        description: 'Manual ALB endpoint (optional - for DNS setup fallback)'
        required: false
        type: string

env:
  AWS_REGION: us-east-1

permissions:
  id-token: write
  contents: read

jobs:
  # Step 1: Prerequisites (Optional)
  prerequisites:
    name: üîß Step 1 - Prerequisites
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'deploy' && !inputs.skip_prerequisites }}
    environment: livekit-poc-${{ inputs.environment }}-prerequisites
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Prerequisites-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Setup Terraform
          wget -q https://releases.hashicorp.com/terraform/1.10.3/terraform_1.10.3_linux_amd64.zip
          unzip -q terraform_1.10.3_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          
          # Setup kubectl
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Setup Helm
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Setup eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v0.191.0/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/

      - name: Run Prerequisites Script
        working-directory: scripts
        run: |
          chmod +x 00-prerequisites.sh
          ./00-prerequisites.sh

  # Step 2: Terraform Plan (Optional)
  terraform-plan:
    name: üìã Step 2 - Terraform Plan
    runs-on: ubuntu-latest
    needs: [prerequisites]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_terraform_plan && (needs.prerequisites.result == 'success' || inputs.skip_prerequisites) }}
    environment: livekit-poc-${{ inputs.environment }}-terraform-plan
    outputs:
      plan-output: ${{ steps.plan.outputs.plan_output }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Terraform-Plan-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform-version: 1.10.3

      - name: Terraform Plan
        id: plan
        working-directory: resources
        run: |
          echo "üìã Running Terraform Plan..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          terraform plan \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -out=tfplan
          
          echo "‚úÖ Terraform plan completed successfully!"

  # Step 3: Terraform Apply (Optional)
  terraform-apply:
    name: üöÄ Step 3 - Terraform Apply
    runs-on: ubuntu-latest
    needs: [terraform-plan]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_terraform_apply && (needs.terraform-plan.result == 'success' || inputs.skip_terraform_plan) }}
    environment: livekit-poc-${{ inputs.environment }}-terraform-apply
    outputs:
      cluster-name: ${{ steps.apply.outputs.cluster_name }}
      redis-endpoint: ${{ steps.apply.outputs.redis_endpoint }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Terraform-Apply-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform-version: 1.10.3

      - name: Terraform Apply
        id: apply
        working-directory: resources
        run: |
          echo "üöÄ Applying Terraform configuration..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          terraform plan \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -out=tfplan
          
          terraform apply tfplan
          
          # Get outputs
          CLUSTER_NAME=$(terraform output -raw cluster_name)
          REDIS_ENDPOINT=$(terraform output -raw redis_cluster_endpoint)
          
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "redis_endpoint=$REDIS_ENDPOINT" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Infrastructure deployed successfully!"
          echo "üìä Cluster: $CLUSTER_NAME"
          echo "üìä Redis: $REDIS_ENDPOINT"

  # Step 4: Configure EKS Access Policies & ACM Certificate
  eks-access-and-acm:
    name: üîê Step 4 - Configure EKS Access & ACM Certificate
    runs-on: ubuntu-latest
    needs: [terraform-apply]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_eks_access && (needs.terraform-apply.result == 'success' || inputs.skip_terraform_apply) }}
    environment: livekit-poc-${{ inputs.environment }}-eks-access-acm
    outputs:
      certificate-arn: ${{ steps.eks-acm-setup.outputs.certificate_arn }}
    
    steps:
      - name: üìã Manual Approval - Configure EKS Access & ACM Certificate
        run: |
          echo "ü§î Manual approval required to configure EKS access policies and ACM certificate"
          echo ""
          echo "üìã This step will:"
          echo "   ‚úÖ Check for existing SSL certificate for domain: ${{ inputs.domain_name }}"
          echo "   ‚úÖ Use existing certificate if valid, or create new one if needed"
          echo "   ‚úÖ Grant AmazonEKSAdminPolicy to pipeline role"
          echo "   ‚úÖ Grant AmazonEKSClusterAdminPolicy to pipeline role"
          echo "   ‚úÖ Update kubeconfig for cluster access"
          echo "   ‚úÖ Create/update DNS validation records in Route53"
          echo "   ‚úÖ Wait for certificate issuance and validation"
          echo "   ‚úÖ Export certificate ARN for use in subsequent steps"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üîó Domain: ${{ inputs.domain_name }}"
          echo "üè∑Ô∏è  Hosted Zone: ${{ inputs.hosted_zone_id }}"
          echo "‚è±Ô∏è Estimated Time: ~5-8 minutes"
          echo ""
          echo "üîí Security: This step requires elevated permissions to:"
          echo "   ‚Ä¢ Manage EKS cluster access policies"
          echo "   ‚Ä¢ Request/manage ACM certificates"
          echo "   ‚Ä¢ Create DNS records in Route53"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-EKS-Access-ACM-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl for cluster verification
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Verify installation
          echo "‚úÖ Tool versions:"
          kubectl version --client

      - name: Configure EKS Access Policies & ACM Certificate
        id: eks-acm-setup
        working-directory: scripts
        run: |
          chmod +x 01-setup-eks-access-and-acm.sh
          
          # Determine cluster name - consistent logic across all steps
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            # Try to get from terraform output, fallback to constructed name
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
            if [ -z "$CLUSTER_NAME" ]; then
              CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
              echo "‚ö†Ô∏è  Using constructed cluster name: $CLUSTER_NAME"
            fi
          fi
          
          echo "üìã Configuration for EKS Access Policy & ACM Certificate:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo "üìã Pipeline Role: ${{ secrets.AWS_OIDC_ROLE_ARN }}"
          echo "üìã Domain: ${{ inputs.domain_name }}"
          echo "üìã Hosted Zone ID: ${{ inputs.hosted_zone_id }}"
          echo ""
          
          # Validate required variables
          if [ -z "$CLUSTER_NAME" ]; then
            echo "‚ùå CLUSTER_NAME could not be determined"
            exit 1
          fi
          
          # Enable full logging
          set -x
          
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          PIPELINE_ROLE_ARN="${{ secrets.AWS_OIDC_ROLE_ARN }}" \
          DOMAIN_NAME="${{ inputs.domain_name }}" \
          HOSTED_ZONE_ID="${{ inputs.hosted_zone_id }}" \
          CERT_REGION="${{ env.AWS_REGION }}" \
          ./01-setup-eks-access-and-acm.sh 2>&1 | tee /tmp/eks-access-acm.log
          
          # Read certificate ARN from output file if it exists
          if [ -f "/tmp/certificate_arn.txt" ]; then
            CERT_ARN=$(cat /tmp/certificate_arn.txt)
            echo "certificate_arn=$CERT_ARN" >> $GITHUB_OUTPUT
            echo "‚úÖ Certificate ARN exported: $CERT_ARN"
          else
            echo "‚ö†Ô∏è  Certificate ARN output file not found"
          fi
          
          echo ""
          echo "üìã EKS Access & ACM Certificate Log Summary:"
          echo "‚úÖ EKS access policy and ACM certificate configuration completed"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 5: Setup AWS Load Balancer Controller
  setup-load-balancer-controller:
    name: üîß Step 5 - Setup AWS Load Balancer Controller
    runs-on: ubuntu-latest
    needs: [eks-access-and-acm, terraform-apply]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_load_balancer && (needs.eks-access-and-acm.result == 'success' || inputs.skip_eks_access) }}
    environment: livekit-poc-${{ inputs.environment }}-setup-load-balancer
    
    steps:
      - name: üìã Manual Approval - Setup Load Balancer Controller
        run: |
          echo "ü§î Manual approval required to setup AWS Load Balancer Controller"
          echo ""
          echo "üìã This step will:"
          echo "   ‚úÖ Install AWS Load Balancer Controller on EKS cluster"
          echo "   ‚úÖ Configure IAM service account with proper permissions"
          echo "   ‚úÖ Setup webhook and admission controller"
          echo "   ‚úÖ Verify controller is running and ready"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üîó Certificate ARN: ${{ needs.eks-access-and-acm.outputs.certificate-arn || 'Will be detected automatically' }}"
          echo "‚è±Ô∏è Estimated Time: ~3-5 minutes"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LoadBalancer-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Install Helm
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v0.191.0/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/
          
          # Install jq and curl
          sudo apt-get update && sudo apt-get install -y jq curl
          
          # Verify installations
          echo "‚úÖ Tool versions:"
          kubectl version --client
          helm version
          eksctl version
          jq --version

      - name: Setup AWS Load Balancer Controller
        working-directory: scripts
        run: |
          chmod +x 02-setup-load-balancer-controller.sh
          
          # Determine cluster name - consistent logic across all steps
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            # Try to get from terraform output, fallback to constructed name
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
            if [ -z "$CLUSTER_NAME" ]; then
              CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
              echo "‚ö†Ô∏è  Using constructed cluster name: $CLUSTER_NAME"
            fi
          fi
          
          echo "üìã Configuration for Load Balancer Controller Setup:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "ÔøΩ Ennvironment: ${{ inputs.environment }}"
          echo "üìã Certificate ARN: ${{ needs.eks-access-and-acm.outputs.certificate-arn || 'Auto-detect' }}"
          echo ""
          
          # Enable full logging and continuous output
          set -x  # Enable command tracing
          
          # Run the Load Balancer Controller setup with full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          CERTIFICATE_ARN="${{ needs.eks-access-and-acm.outputs.certificate-arn }}" \
          ./02-setup-load-balancer-controller.sh 2>&1 | tee /tmp/lbc-setup.log
          
          # Show final status
          echo ""
          echo "üîç Final Status Check:"
          kubectl get deployment aws-load-balancer-controller -n kube-system || echo "Deployment not found"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "No pods found"
          kubectl get service aws-load-balancer-webhook-service -n kube-system || echo "Webhook service not found"
          
          echo ""
          echo "üìã Setup Log Summary:"
          echo "‚úÖ Load Balancer Controller setup completed"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 6: Deploy LiveKit
  deploy-livekit:
    name: üé• Step 6 - Deploy LiveKit
    runs-on: ubuntu-latest
    needs: [setup-load-balancer-controller, eks-access-and-acm, terraform-apply]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_livekit && (needs.setup-load-balancer-controller.result == 'success' || inputs.skip_load_balancer) }}
    environment: livekit-poc-${{ inputs.environment }}-deploy-livekit
    
    steps:
      - name: üìã Manual Approval - Deploy LiveKit
        run: |
          echo "ü§î Manual approval required to deploy LiveKit"
          echo ""
          echo "üìã This step will deploy:"
          echo "   ‚úÖ LiveKit Server using official Helm chart"
          echo "   ‚úÖ ALB Ingress with SSL certificate"
          echo "   ‚úÖ TURN server for WebRTC connectivity"
          echo "   ‚úÖ Metrics and Prometheus monitoring"
          echo "   ‚úÖ Redis integration for clustering"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üîó Domain: ${{ inputs.domain_name }}"
          echo "üîó Certificate ARN: ${{ needs.eks-access-and-acm.outputs.certificate-arn || 'Auto-detect by domain' }}"
          echo "‚è±Ô∏è Estimated Time: ~5-10 minutes"
          echo ""
          echo "üîç Certificate Handling:"
          echo "   ‚Ä¢ If certificate ARN provided: Uses specified certificate"
          echo "   ‚Ä¢ If certificate ARN missing: Auto-detects by domain name"
          echo "   ‚Ä¢ Validates certificate is in ISSUED status"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Deploy-LiveKit-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install jq for JSON processing
          sudo apt-get update && sudo apt-get install -y jq
          
          # Verify installations
          echo "‚úÖ Tool versions:"
          kubectl version --client
          helm version
          jq --version

      - name: Deploy LiveKit
        working-directory: scripts
        run: |
          chmod +x 03-deploy-livekit.sh
          
          # Determine cluster name - consistent with other steps
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            # Try to get from terraform output, fallback to constructed name
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
            if [ -z "$CLUSTER_NAME" ]; then
              CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
              echo "‚ö†Ô∏è  Using constructed cluster name: $CLUSTER_NAME"
            fi
          fi
          
          # Determine Redis endpoint - consistent with other steps
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            REDIS_ENDPOINT="lp-ec-redis-use1-${{ inputs.environment }}-redis.x4ncn3.ng.0001.use1.cache.amazonaws.com:6379"
          else
            # Try to get from terraform output, fallback to constructed name
            REDIS_ENDPOINT="${{ needs.terraform-apply.outputs.redis-endpoint }}"
            if [ -z "$REDIS_ENDPOINT" ]; then
              REDIS_ENDPOINT="lp-ec-redis-use1-${{ inputs.environment }}-redis.x4ncn3.ng.0001.use1.cache.amazonaws.com:6379"
              echo "‚ö†Ô∏è  Using constructed Redis endpoint: $REDIS_ENDPOINT"
            fi
          fi
          
          echo "üìã Configuration for LiveKit Deployment:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Redis: $REDIS_ENDPOINT"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo "üìã Domain: ${{ inputs.domain_name }}"
          echo "üìã Certificate ARN: ${{ needs.eks-access-and-acm.outputs.certificate-arn || 'Auto-detect by domain' }}"
          echo "üìã Certificate Region: ${{ env.AWS_REGION }}"
          echo ""
          
          # Validate required variables
          if [ -z "$CLUSTER_NAME" ]; then
            echo "‚ùå CLUSTER_NAME could not be determined"
            exit 1
          fi
          
          if [ -z "$REDIS_ENDPOINT" ]; then
            echo "‚ùå REDIS_ENDPOINT could not be determined"
            exit 1
          fi
          
          # Enable full logging and continuous output
          set -x  # Enable command tracing
          
          # Deploy LiveKit with configuration and full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          REDIS_ENDPOINT="$REDIS_ENDPOINT" \
          ENVIRONMENT="${{ inputs.environment }}" \
          DOMAIN_NAME="${{ inputs.domain_name }}" \
          CERTIFICATE_ARN="${{ needs.eks-access-and-acm.outputs.certificate-arn }}" \
          CERT_REGION="${{ env.AWS_REGION }}" \
          ./03-deploy-livekit.sh 2>&1 | tee /tmp/livekit-deployment.log
          
          # Show final status
          echo ""
          echo "ÔøΩ  Final Status Check:"
          kubectl get deployment -n livekit || echo "No deployments found"
          kubectl get pods -n livekit || echo "No pods found"
          kubectl get ingress -n livekit || echo "No ingress found"
          kubectl get services -n livekit || echo "No services found"
          
          echo ""
          echo "üìã Deployment Log Summary:"
          echo "‚úÖ LiveKit deployment completed"
          echo "üîó Access your LiveKit server at: https://${{ inputs.domain_name }}"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 7: Setup DNS Records for LiveKit
  setup-dns-records:
    name: üåê Step 7 - Setup DNS Records for LiveKit
    runs-on: ubuntu-latest
    needs: [deploy-livekit, eks-access-and-acm, terraform-apply]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_dns_setup && ((needs.deploy-livekit.result == 'success') || (inputs.skip_livekit && needs.eks-access-and-acm.result == 'success')) }}
    environment: livekit-poc-${{ inputs.environment }}-setup-dns
    outputs:
      alb-endpoint: ${{ steps.dns-setup.outputs.alb_endpoint }}
      primary-domain: ${{ steps.dns-setup.outputs.primary_domain }}
      turn-domain: ${{ steps.dns-setup.outputs.turn_domain }}
    
    steps:
      - name: üìã Manual Approval - Setup DNS Records
        run: |
          echo "ü§î Manual approval required to setup DNS records for LiveKit"
          echo ""
          echo "üìã This step will:"
          echo "   ‚úÖ Wait 5 minutes for ALB to be fully active and stable (if needed)"
          echo "   ‚úÖ Retrieve ALB endpoint from LiveKit ingress (with fallback methods)"
          echo "   ‚úÖ Create primary domain A record (ALIAS): ${{ inputs.domain_name }}"
          echo "   ‚úÖ Create TURN domain A record (ALIAS): turn-${{ inputs.domain_name }}"
          echo "   ‚úÖ Handle existing DNS records properly (delete/recreate)"
          echo "   ‚úÖ Verify DNS resolution and propagation"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üîó Primary Domain: ${{ inputs.domain_name }}"
          echo "üîÑ TURN Domain: turn-${{ inputs.domain_name }}"
          echo "üè∑Ô∏è  Hosted Zone: ${{ inputs.hosted_zone_id }}"
          echo "üîó Manual ALB Endpoint: ${{ inputs.manual_alb_endpoint || 'Auto-detect' }}"
          echo "‚è±Ô∏è Estimated Time: ~8-10 minutes (includes 5min ALB wait if needed)"
          echo ""
          echo "üîç ALB Detection Methods:"
          echo "   ‚Ä¢ Method 1: Get from LiveKit ingress status"
          echo "   ‚Ä¢ Method 2: Search for latest LiveKit ALB by creation time"
          echo "   ‚Ä¢ Method 3: Find any LiveKit ALB by name"
          echo "   ‚Ä¢ Method 4: Use manual ALB endpoint if provided"
          echo ""
          echo "üìã DNS Record Type: A record (ALIAS) - points directly to ALB"
          echo ""
          echo "üîí Security: This step requires permissions to:"
          echo "   ‚Ä¢ Read Kubernetes ingress resources"
          echo "   ‚Ä¢ List AWS Load Balancers"
          echo "   ‚Ä¢ Manage Route53 DNS records"
          echo ""
          echo "‚ÑπÔ∏è  Note: You can skip this step using 'Skip DNS Records Setup' option"
          echo "‚ÑπÔ∏è  This step can run independently if LiveKit is already deployed"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-DNS-Setup-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl for ingress inspection
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Install additional tools for DNS testing
          sudo apt-get update && sudo apt-get install -y dnsutils curl
          
          # Verify installations
          echo "‚úÖ Tool versions:"
          kubectl version --client
          nslookup -version || echo "nslookup available"

      - name: Setup DNS Records for LiveKit
        id: dns-setup
        working-directory: scripts
        run: |
          chmod +x 04-setup-dns-records.sh
          
          # Determine cluster name - handle case where terraform steps were skipped
          if [ "${{ inputs.skip_terraform_apply }}" = "true" ]; then
            CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          else
            # Try to get from terraform output, fallback to constructed name
            CLUSTER_NAME="${{ needs.terraform-apply.outputs.cluster-name }}"
            if [ -z "$CLUSTER_NAME" ]; then
              CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
              echo "‚ö†Ô∏è  Using constructed cluster name: $CLUSTER_NAME"
            fi
          fi
          
          echo "üìã Configuration for DNS Records Setup:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo "üìã Primary Domain: ${{ inputs.domain_name }}"
          echo "üìã TURN Domain: turn-${{ inputs.domain_name }}"
          echo "üìã Hosted Zone ID: ${{ inputs.hosted_zone_id }}"
          echo "üìã Manual ALB Endpoint: ${{ inputs.manual_alb_endpoint || 'Auto-detect' }}"
          echo ""
          
          # Validate required variables
          if [ -z "$CLUSTER_NAME" ]; then
            echo "‚ùå CLUSTER_NAME could not be determined"
            exit 1
          fi
          
          # Enable full logging
          set -x
          
          # Run DNS setup with full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          DOMAIN_NAME="${{ inputs.domain_name }}" \
          HOSTED_ZONE_ID="${{ inputs.hosted_zone_id }}" \
          MANUAL_ALB_ENDPOINT="${{ inputs.manual_alb_endpoint }}" \
          ./04-setup-dns-records.sh 2>&1 | tee /tmp/dns-setup.log
          
          # Read outputs from files if they exist
          if [ -f "/tmp/alb_endpoint.txt" ]; then
            ALB_ENDPOINT=$(cat /tmp/alb_endpoint.txt)
            echo "alb_endpoint=$ALB_ENDPOINT" >> $GITHUB_OUTPUT
            echo "primary_domain=${{ inputs.domain_name }}" >> $GITHUB_OUTPUT
            echo "turn_domain=turn-${{ inputs.domain_name }}" >> $GITHUB_OUTPUT
            echo "‚úÖ DNS information exported: $ALB_ENDPOINT"
          else
            echo "‚ö†Ô∏è  ALB endpoint output file not found"
          fi
          
          echo ""
          echo "üìã DNS Setup Log Summary:"
          echo "‚úÖ DNS records setup completed"
          echo "üåê Primary Domain: https://${{ inputs.domain_name }}"
          echo "üîÑ TURN Domain: turn-${{ inputs.domain_name }}"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 8: Deploy SIP Server
  deploy-sip-server:
    name: üìû Step 8 - Deploy SIP Server
    runs-on: ubuntu-latest
    needs: [ setup-dns-records, deploy-livekit, eks-access-and-acm]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_sip_server && ((needs.deploy-livekit.result == 'success' || inputs.skip_livekit) && (needs.eks-access-and-acm.result == 'success' || inputs.skip_eks_access)) }}
    environment: livekit-poc-${{ inputs.environment }}-deploy-sip-server
    
    steps:
      - name: üìã Manual Approval - Deploy SIP Server
        run: |
          echo "ü§î Manual approval required to deploy SIP Server"
          echo ""
          echo "üìã This step will deploy:"
          echo "   ‚úÖ SIP ConfigMap with dynamic configuration"
          echo "   ‚úÖ SIP Server deployment with proper resource limits"
          echo "   ‚úÖ SIP Service for UDP/TCP connectivity"
          echo "   ‚úÖ Health checks and monitoring"
          echo "   ‚úÖ Integration with LiveKit and Redis"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üîó Domain: ${{ inputs.domain_name }}"
          echo "üìû SIP Port: 5060"
          echo "üîÑ RTP Port Range: 10000-20000"
          echo "‚è±Ô∏è Estimated Time: ~3-5 minutes"
          echo ""
          echo "üìã SIP Server Features:"
          echo "   ‚Ä¢ WebSocket URL: wss://${{ inputs.domain_name }}"
          echo "   ‚Ä¢ Redis integration for clustering"
          echo "   ‚Ä¢ Host networking for RTP traffic"
          echo "   ‚Ä¢ Health checks and auto-restart"
          echo "   ‚Ä¢ Production resource limits"
          echo ""
          echo "üîí Security: This step requires permissions to:"
          echo "   ‚Ä¢ Create Kubernetes resources (ConfigMap, Deployment, Service)"
          echo "   ‚Ä¢ Access LiveKit namespace"
          echo ""
          echo "‚ÑπÔ∏è  Note: This step requires EKS access and LiveKit namespace to exist"
          echo "‚ÑπÔ∏è  Dependencies: EKS Access & ACM step, LiveKit deployment (or namespace)"
          echo "‚ÑπÔ∏è  You can skip this step using 'Skip SIP Server Deployment' option"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-SIP-Server-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl for Kubernetes operations
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Verify installation
          echo "‚úÖ Tool versions:"
          kubectl version --client

      - name: Deploy SIP Server
        working-directory: scripts
        run: |
          chmod +x 05-deploy-sip-server.sh
          
          # Use constructed values since SIP server runs independently
          CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          REDIS_ENDPOINT="lp-ec-redis-use1-${{ inputs.environment }}-redis.x4ncn3.ng.0001.use1.cache.amazonaws.com:6379"
          
          echo "üìã Configuration for SIP Server Deployment:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo "üìã Domain: ${{ inputs.domain_name }}"
          echo "üìã Redis Endpoint: $REDIS_ENDPOINT"
          echo ""
          
          # Enable full logging
          set -x
          
          # Deploy SIP Server with configuration and full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          DOMAIN_NAME="${{ inputs.domain_name }}" \
          REDIS_ENDPOINT="$REDIS_ENDPOINT" \
          ./05-deploy-sip-server.sh 2>&1 | tee /tmp/sip-server-deployment.log
          
          # Show final status
          echo ""
          echo "üîç Final Status Check:"
          kubectl get all -n livekit -l app=sip-server || echo "No SIP server resources found"
          
          echo ""
          echo "üìã SIP Server Deployment Log Summary:"
          echo "‚úÖ SIP Server deployment completed"
          echo "üìû SIP Server: Ready at ${{ inputs.domain_name }}:5060"
          echo "üìÑ Full logs available in pipeline output above"

  # Step 9: Deploy Kamailio SIP Load Balancer
  deploy-kamailio:
    name: üìû Step 9 - Deploy Kamailio SIP Load Balancer
    runs-on: ubuntu-latest
    needs: [deploy-sip-server, eks-access-and-acm]
    if: ${{ always() && inputs.action == 'deploy' && !inputs.skip_kamailio && ((needs.deploy-sip-server.result == 'success' || inputs.skip_sip_server) && (needs.eks-access-and-acm.result == 'success' || inputs.skip_eks_access)) }}
    environment: livekit-poc-${{ inputs.environment }}-deploy-kamailio
    
    steps:
      - name: üìã Manual Approval - Deploy Kamailio SIP Load Balancer
        run: |
          echo "ü§î Manual approval required to deploy Kamailio SIP Load Balancer"
          echo ""
          echo "üìã This step will deploy:"
          echo "   ‚úÖ Kamailio ConfigMap with SIP routing configuration"
          echo "   ‚úÖ RBAC resources for dispatchers service account"
          echo "   ‚úÖ Kamailio deployment with dispatchers sidecar"
          echo "   ‚úÖ AWS Network Load Balancer (NLB) for internet-facing SIP traffic"
          echo "   ‚úÖ Health checks and monitoring for Kamailio"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üìû SIP Port: 5060 (UDP/TCP)"
          echo "üîÑ Load Balancer: AWS NLB (internet-facing)"
          echo "‚è±Ô∏è Estimated Time: ~5-8 minutes"
          echo ""
          echo "üìã Kamailio Features:"
          echo "   ‚Ä¢ SIP load balancing and routing"
          echo "   ‚Ä¢ Automatic backend discovery via dispatchers"
          echo "   ‚Ä¢ Failover and health monitoring"
          echo "   ‚Ä¢ Internet-facing NLB for Twilio/external SIP traffic"
          echo "   ‚Ä¢ UDP and TCP SIP support"
          echo ""
          echo "üîí Security: This step requires permissions to:"
          echo "   ‚Ä¢ Create Kubernetes resources (ConfigMap, Deployment, Service, RBAC)"
          echo "   ‚Ä¢ Create AWS Network Load Balancer"
          echo "   ‚Ä¢ Access LiveKit namespace"
          echo ""
          echo "‚ÑπÔ∏è  Note: This step requires EKS access and LiveKit namespace to exist"
          echo "‚ÑπÔ∏è  Dependencies: EKS Access & ACM step, SIP Server deployment (or namespace)"
          echo "‚ÑπÔ∏è  You can skip this step using 'Skip Kamailio SIP Load Balancer Deployment' option"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-Kamailio-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl for Kubernetes operations
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Verify installation
          echo "‚úÖ Tool versions:"
          kubectl version --client

      - name: Deploy Kamailio SIP Load Balancer
        working-directory: scripts
        run: |
          chmod +x 06-deploy-kamailio.sh
          
          # Use constructed values since Kamailio runs independently
          CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          
          echo "üìã Configuration for Kamailio Deployment:"
          echo "üìã Cluster: $CLUSTER_NAME"
          echo "üìã Region: ${{ env.AWS_REGION }}"
          echo "üìã Environment: ${{ inputs.environment }}"
          echo "üìã SIP Port: 5060"
          echo ""
          
          # Enable full logging
          set -x
          
          # Deploy Kamailio with configuration and full logging
          CLUSTER_NAME="$CLUSTER_NAME" \
          AWS_REGION="${{ env.AWS_REGION }}" \
          ENVIRONMENT="${{ inputs.environment }}" \
          ./06-deploy-kamailio.sh 2>&1 | tee /tmp/kamailio-deployment.log
          
          # Show final status
          echo ""
          echo "üîç Final Status Check:"
          kubectl get all -n livekit -l app=kamailio || echo "No Kamailio resources found"
          kubectl get service kamailio -n livekit || echo "No Kamailio service found"
          
          echo ""
          echo "üìã Kamailio Deployment Log Summary:"
          echo "‚úÖ Kamailio SIP Load Balancer deployment completed"
          echo "üìû Kamailio: Ready for SIP load balancing"
          echo "üåê NLB: Internet-facing load balancer created"
          echo "üìÑ Full logs available in pipeline output above"

  # DESTROY: Remove everything (Optional)
  destroy:
    name: üóëÔ∏è Destroy LiveKit Infrastructure
    runs-on: ubuntu-latest
    if: ${{ inputs.action == 'destroy' }}
    environment: livekit-poc-${{ inputs.environment }}-destroy
    
    steps:
      - name: üìã Manual Approval - Destroy Infrastructure
        run: |
          echo "ü§î Manual approval required to destroy LiveKit infrastructure"
          echo ""
          echo "‚ö†Ô∏è  WARNING: This will permanently delete:"
          echo "   üóëÔ∏è  All Kubernetes resources (LiveKit, SIP Server, Load Balancer Controller)"
          echo "   üóëÔ∏è  EKS cluster and associated resources"
          echo "   üóëÔ∏è  ElastiCache Redis cluster"
          echo "   üóëÔ∏è  VPC, subnets, and networking components"
          echo "   üóëÔ∏è  DNS records for ${{ inputs.domain_name }} and turn-${{ inputs.domain_name }}"
          echo "   üóëÔ∏è  All Terraform-managed infrastructure"
          echo ""
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "‚è±Ô∏è Estimated Time: ~10-15 minutes"
          echo ""
          echo "üîí This action cannot be undone!"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-Destroy-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          wget -q https://releases.hashicorp.com/terraform/1.10.3/terraform_1.10.3_linux_amd64.zip
          unzip -q terraform_1.10.3_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          
          curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          curl -fsSL https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/

      - name: üóëÔ∏è Clean up DNS Records
        run: |
          echo "üóëÔ∏è Cleaning up DNS records before infrastructure destruction..."
          
          # Function to delete DNS record if it exists
          delete_dns_record_if_exists() {
            local record_name="$1"
            local record_type="$2"
            
            echo "üîç Checking for $record_type record: $record_name"
            
            if [[ "$record_type" == "A" ]]; then
              # Get existing A record (ALIAS)
              local existing_value=$(aws route53 list-resource-record-sets \
                --hosted-zone-id "${{ inputs.hosted_zone_id }}" \
                --query "ResourceRecordSets[?Name=='${record_name}.' && Type=='${record_type}'].AliasTarget.DNSName" \
                --output text 2>/dev/null || echo "")
              
              if [[ -n "$existing_value" && "$existing_value" != "None" ]]; then
                echo "üóëÔ∏è  Deleting $record_type record: $record_name -> $existing_value"
                
                # Get ALB zone ID for deletion
                local alb_zone_id=$(aws elbv2 describe-load-balancers \
                  --query "LoadBalancers[?DNSName=='$existing_value'].CanonicalHostedZoneId" \
                  --output text 2>/dev/null || echo "")
                
                if [[ -n "$alb_zone_id" && "$alb_zone_id" != "None" ]]; then
                  cat <<EOF > delete-record.json
          {
              "Comment": "Cleanup $record_type record for $record_name during destroy",
              "Changes": [
                  {
                      "Action": "DELETE",
                      "ResourceRecordSet": {
                          "Name": "$record_name",
                          "Type": "$record_type",
                          "AliasTarget": {
                              "DNSName": "$existing_value",
                              "EvaluateTargetHealth": true,
                              "HostedZoneId": "$alb_zone_id"
                          }
                      }
                  }
              ]
          }
          EOF
                  
                  aws route53 change-resource-record-sets \
                    --hosted-zone-id "${{ inputs.hosted_zone_id }}" \
                    --change-batch file://delete-record.json || echo "Failed to delete $record_name"
                  
                  rm -f delete-record.json
                  echo "‚úÖ $record_type record deleted: $record_name"
                else
                  echo "‚ö†Ô∏è  Could not get ALB zone ID for deletion"
                fi
              else
                echo "‚ÑπÔ∏è  No $record_type record found for: $record_name"
              fi
            fi
          }
          
          # Delete primary domain record
          delete_dns_record_if_exists "${{ inputs.domain_name }}" "A"
          
          # Delete TURN domain record
          delete_dns_record_if_exists "turn-${{ inputs.domain_name }}" "A"
          
          echo "‚úÖ DNS cleanup completed"

      - name: üóëÔ∏è Clean up Kubernetes Resources
        run: |
          echo "üóëÔ∏è Cleaning up Kubernetes resources before infrastructure destruction..."
          
          # Update kubeconfig to access the cluster
          CLUSTER_NAME="lp-eks-livekit-use1-${{ inputs.environment }}"
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "${{ env.AWS_REGION }}" || echo "Failed to update kubeconfig"
          
          # Delete SIP Server resources
          echo "üóëÔ∏è Cleaning up SIP Server resources..."
          kubectl delete deployment sip-server -n livekit --ignore-not-found=true || echo "SIP deployment cleanup failed"
          kubectl delete service sip-server -n livekit --ignore-not-found=true || echo "SIP service cleanup failed"
          kubectl delete configmap sip-config -n livekit --ignore-not-found=true || echo "SIP configmap cleanup failed"
          
          # Delete Kamailio resources
          echo "üóëÔ∏è Cleaning up Kamailio resources..."
          kubectl delete deployment kamailio -n livekit --ignore-not-found=true || echo "Kamailio deployment cleanup failed"
          kubectl delete service kamailio -n livekit --ignore-not-found=true || echo "Kamailio service cleanup failed"
          kubectl delete configmap kamailio-config -n livekit --ignore-not-found=true || echo "Kamailio configmap cleanup failed"
          kubectl delete rolebinding dispatchers-rolebinding -n livekit --ignore-not-found=true || echo "Kamailio rolebinding cleanup failed"
          kubectl delete role dispatchers-role -n livekit --ignore-not-found=true || echo "Kamailio role cleanup failed"
          kubectl delete serviceaccount dispatchers -n livekit --ignore-not-found=true || echo "Kamailio serviceaccount cleanup failed"
          
          # Delete LiveKit resources
          echo "üóëÔ∏è Cleaning up LiveKit resources..."
          kubectl delete namespace livekit --ignore-not-found=true || echo "LiveKit namespace cleanup failed"
          
          echo "‚úÖ Kubernetes cleanup completed"

      - name: üóëÔ∏è Destroy All Infrastructure
        working-directory: resources
        run: |
          echo "üóëÔ∏è Destroying all infrastructure with Terraform..."
          
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          # Run destroy with retries
          MAX_ATTEMPTS=3
          ATTEMPT=1
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "üîÑ Destroy attempt $ATTEMPT/$MAX_ATTEMPTS..."
            
            if terraform destroy \
              -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
              -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
              -auto-approve; then
              echo "‚úÖ All infrastructure destroyed!"
              exit 0
            else
              echo "‚ö†Ô∏è Destroy attempt $ATTEMPT failed"
              
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "‚ùå Destroy failed after $MAX_ATTEMPTS attempts"
                exit 1
              fi
              
              echo "‚è≥ Waiting 10 seconds before retry..."
              sleep 10
              ATTEMPT=$((ATTEMPT + 1))
            fi
          done