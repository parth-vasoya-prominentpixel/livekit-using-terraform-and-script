name: LiveKit EKS Hybrid Deployment (eksctl + Terraform)

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - uat
          - prod
      step:
        description: 'Deployment step to execute'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - create-cluster
          - deploy-resources
          - setup-load-balancer
          - deploy-livekit
          - destroy

env:
  AWS_REGION: us-east-1
  TERRAFORM_VERSION: 1.10.3
  KUBECTL_VERSION: v1.32.0
  HELM_VERSION: v3.16.3
  EKSCTL_VERSION: 0.197.0

permissions:
  id-token: write
  contents: read

jobs:
  # Step 1: Create EKS Cluster with eksctl
  create-eks-cluster:
    name: üöÄ Step 1 - Create EKS Cluster (eksctl)
    runs-on: ubuntu-latest
    if: ${{ inputs.step == 'all' || inputs.step == 'create-cluster' }}
    environment: 
      name: ${{ inputs.environment }}-create-cluster
    outputs:
      cluster-name: ${{ steps.create-cluster.outputs.cluster_name }}
    
    steps:
      - name: üìã Manual Approval - Create EKS Cluster
        run: |
          echo "ü§î Manual approval required for EKS Cluster Creation"
          echo "‚ö†Ô∏è  WARNING: This will create AWS resources that may incur costs!"
          echo "üìã Resources to be created:"
          echo "   - EKS Cluster: livekit-cluster-v2"
          echo "   - Kubernetes Version: 1.34"
          echo "   - Node Type: t3.medium (3 nodes: 2 min, 3 max)"
          echo "   - VPC and networking (auto-created by eksctl)"
          echo "   - OIDC provider, IAM roles, security groups"
          echo "üåç Environment: ${{ inputs.environment }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-EKS-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v${{ env.EKSCTL_VERSION }}/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/
          eksctl version
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client
          
          # Install jq
          sudo apt-get update && sudo apt-get install -y jq

      - name: Create EKS Cluster
        id: create-cluster
        working-directory: scripts
        run: |
          echo "üöÄ Creating EKS cluster with eksctl..."
          chmod +x 01-create-eks-cluster.sh
          ./01-create-eks-cluster.sh
          
          # Get cluster name from the created file
          CLUSTER_NAME=$(jq -r '.cluster_name' ../terraform-cluster-info.json)
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          
          echo "‚úÖ EKS cluster created successfully!"
          echo "üè∑Ô∏è Cluster Name: $CLUSTER_NAME"

  # Step 2: Deploy Redis and Security Groups with Terraform
  deploy-additional-resources:
    name: üîß Step 2 - Deploy Redis & Security Groups
    runs-on: ubuntu-latest
    needs: [create-eks-cluster]
    if: ${{ always() && (needs.create-eks-cluster.result == 'success' || inputs.step == 'deploy-resources') }}
    environment: 
      name: ${{ inputs.environment }}-terraform
    outputs:
      redis-endpoint: ${{ steps.get-outputs.outputs.redis_endpoint }}
    
    steps:
      - name: üìã Manual Approval - Deploy Additional Resources
        run: |
          echo "ü§î Manual approval required for Additional Resources"
          echo "üìã Resources to be created:"
          echo "   - ElastiCache Redis cluster"
          echo "   - Security Groups for SIP traffic (port 5060 from Twilio only)"
          echo "   - Attach SIP security group to EKS nodes"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üè∑Ô∏è EKS Cluster: ${{ needs.create-eks-cluster.outputs.cluster-name || 'livekit-cluster-v2' }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-Terraform-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install Terraform
          wget -q https://releases.hashicorp.com/terraform/${{ env.TERRAFORM_VERSION }}/terraform_${{ env.TERRAFORM_VERSION }}_linux_amd64.zip
          unzip -q terraform_${{ env.TERRAFORM_VERSION }}_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          terraform version
          
          # Install jq
          sudo apt-get update && sudo apt-get install -y jq

      - name: Create cluster info file (if not from previous step)
        if: ${{ inputs.step == 'deploy-resources' }}
        run: |
          # Create cluster info file for standalone resource deployment
          cat > terraform-cluster-info.json << EOF
          {
            "cluster_name": "livekit-cluster-v2",
            "vpc_id": "$(aws eks describe-cluster --name livekit-cluster-v2 --region ${{ env.AWS_REGION }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)",
            "cluster_security_group_id": "$(aws eks describe-cluster --name livekit-cluster-v2 --region ${{ env.AWS_REGION }} --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text)",
            "region": "${{ env.AWS_REGION }}"
          }
          EOF

      - name: Deploy Additional Resources
        working-directory: resources
        run: |
          echo "üîß Initializing Terraform..."
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars"
          
          echo "üîç Validating configuration..."
          terraform validate
          
          echo "üìã Creating plan..."
          terraform plan \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -out=tfplan
          
          echo "üöÄ Applying additional resources..."
          terraform apply tfplan
          
          echo "‚úÖ Additional resources deployed successfully!"

      - name: Attach SIP Security Group
        working-directory: scripts
        run: |
          echo "üîí Attaching SIP security group to EKS nodes..."
          chmod +x 02-attach-sip-security-group.sh
          ./02-attach-sip-security-group.sh
          
          echo "‚úÖ SIP security group attached successfully!"

      - name: Get Terraform Outputs
        id: get-outputs
        working-directory: resources
        run: |
          echo "üìä Getting Terraform outputs..."
          REDIS_ENDPOINT=$(terraform output -raw redis_cluster_endpoint)
          
          echo "redis_endpoint=$REDIS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "üîó Redis Endpoint: $REDIS_ENDPOINT"

  # Step 3: Setup Load Balancer Controller
  setup-load-balancer:
    name: ‚öñÔ∏è Step 3 - Setup Load Balancer Controller
    runs-on: ubuntu-latest
    needs: [create-eks-cluster, deploy-additional-resources]
    if: ${{ always() && (needs.deploy-additional-resources.result == 'success' || inputs.step == 'setup-load-balancer') }}
    environment: 
      name: ${{ inputs.environment }}-load-balancer
    
    steps:
      - name: üìã Manual Approval - Load Balancer Controller
        run: |
          echo "ü§î Manual approval required for Load Balancer Controller Setup"
          echo "üìã This step will install AWS Load Balancer Controller"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üè∑Ô∏è Cluster: ${{ needs.create-eks-cluster.outputs.cluster-name || 'livekit-cluster-v2' }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-LB-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Install Helm
          curl -fsSL https://get.helm.sh/helm-${{ env.HELM_VERSION }}-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/
          
          # Install eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v${{ env.EKSCTL_VERSION }}/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/

      - name: Setup Load Balancer Controller
        run: |
          echo "‚öñÔ∏è Setting up AWS Load Balancer Controller..."
          
          CLUSTER_NAME="${{ needs.create-eks-cluster.outputs.cluster-name || 'livekit-cluster-v2' }}"
          
          # Configure kubectl
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name "$CLUSTER_NAME"
          kubectl get nodes
          
          # Download IAM policy for Load Balancer Controller
          curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
          
          # Create IAM policy
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam_policy.json || true
          
          # Create service account for Load Balancer Controller
          eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn=arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy \
            --approve \
            --region=${{ env.AWS_REGION }}
          
          # Install AWS Load Balancer Controller
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }}
          
          # Wait for deployment
          kubectl wait --for=condition=available --timeout=300s deployment/aws-load-balancer-controller -n kube-system
          
          echo "‚úÖ Load Balancer Controller setup completed!"

  # Step 4: Deploy LiveKit
  deploy-livekit:
    name: üé• Step 4 - Deploy LiveKit
    runs-on: ubuntu-latest
    needs: [create-eks-cluster, deploy-additional-resources, setup-load-balancer]
    if: ${{ always() && (needs.setup-load-balancer.result == 'success' || inputs.step == 'deploy-livekit') }}
    environment: 
      name: ${{ inputs.environment }}-livekit
    
    steps:
      - name: üìã Manual Approval - LiveKit Deployment
        run: |
          echo "ü§î Manual approval required for LiveKit Deployment"
          echo "üìã This step will deploy LiveKit application"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üè∑Ô∏è Cluster: ${{ needs.create-eks-cluster.outputs.cluster-name || 'livekit-cluster-v2' }}"
          echo "üîó Redis: ${{ needs.deploy-additional-resources.outputs.redis-endpoint }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-Deploy-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          # Install Helm
          curl -fsSL https://get.helm.sh/helm-${{ env.HELM_VERSION }}-linux-amd64.tar.gz | tar -xz
          sudo mv linux-amd64/helm /usr/local/bin/

      - name: Deploy LiveKit
        run: |
          echo "üé• Deploying LiveKit..."
          
          CLUSTER_NAME="${{ needs.create-eks-cluster.outputs.cluster-name || 'livekit-cluster-v2' }}"
          REDIS_ENDPOINT="${{ needs.deploy-additional-resources.outputs.redis-endpoint }}"
          
          # Configure kubectl
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name "$CLUSTER_NAME"
          
          # Create namespace
          kubectl create namespace livekit --dry-run=client -o yaml | kubectl apply -f -
          
          # Add LiveKit Helm repo
          helm repo add livekit https://livekit.github.io/charts
          helm repo update
          
          # Update Redis endpoint in values file
          sed -i "s|redis:.*|redis: \"$REDIS_ENDPOINT\"|g" livekit-values.yaml
          
          # Deploy LiveKit
          helm upgrade --install livekit livekit/livekit \
            -n livekit \
            -f livekit-values.yaml \
            --wait --timeout=10m
          
          # Show status
          kubectl get pods -n livekit
          kubectl get svc -n livekit
          kubectl get ingress -n livekit
          
          echo "‚úÖ LiveKit deployed successfully!"

  # Step 5: Destroy Infrastructure
  destroy-infrastructure:
    name: üóëÔ∏è Destroy Infrastructure
    runs-on: ubuntu-latest
    if: ${{ inputs.step == 'destroy' }}
    environment: 
      name: ${{ inputs.environment }}-destroy
    
    steps:
      - name: üìã Manual Approval - Destroy Infrastructure
        run: |
          echo "ü§î Manual approval required for Infrastructure Destruction"
          echo "‚ö†Ô∏è  DANGER: This will permanently delete ALL resources!"
          echo "üìã Resources to be destroyed:"
          echo "   - EKS Cluster and all workloads"
          echo "   - VPC and networking"
          echo "   - ElastiCache Redis"
          echo "   - All security groups and IAM roles"
          echo "üåç Environment: ${{ inputs.environment }}"
          echo "üö® ALL DATA WILL BE PERMANENTLY LOST!"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          role-session-name: GitHubActions-LiveKit-Destroy-${{ github.run_id }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        run: |
          # Install Terraform
          wget -q https://releases.hashicorp.com/terraform/${{ env.TERRAFORM_VERSION }}/terraform_${{ env.TERRAFORM_VERSION }}_linux_amd64.zip
          unzip -q terraform_${{ env.TERRAFORM_VERSION }}_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          
          # Install eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/download/v${{ env.EKSCTL_VERSION }}/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin/
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

      - name: Cleanup and Destroy
        run: |
          echo "üóëÔ∏è Destroying infrastructure..."
          
          CLUSTER_NAME="livekit-cluster-v2"
          
          # Cleanup Kubernetes resources first
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name "$CLUSTER_NAME" || true
          kubectl delete namespace livekit --ignore-not-found=true --timeout=60s || true
          helm uninstall aws-load-balancer-controller -n kube-system || true
          sleep 30
          
          # Destroy Terraform resources
          cd resources
          terraform init -backend-config="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/backend.tfvars" || true
          terraform destroy \
            -var-file="../environments/livekit-poc/${{ env.AWS_REGION }}/${{ inputs.environment }}/inputs.tfvars" \
            -var="deployment_role_arn=${{ secrets.DEPLOYMENT_ROLE_ARN }}" \
            -auto-approve || true
          
          # Delete EKS cluster
          eksctl delete cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          
          echo "‚úÖ Infrastructure destroyed successfully!"