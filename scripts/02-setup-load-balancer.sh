#!/bin/bash

# Script to setup AWS Load Balancer Controller on EKS
# This script is idempotent - safe to run multiple times

echo "âš–ï¸ Setting up AWS Load Balancer Controller..."

# Check if CLUSTER_NAME is provided
if [ -z "$CLUSTER_NAME" ]; then
    echo "âŒ CLUSTER_NAME environment variable is required"
    echo "Usage: CLUSTER_NAME=your-cluster-name ./02-setup-load-balancer.sh"
    exit 1
fi

# Set AWS region (default to us-east-1 if not set)
AWS_REGION=${AWS_REGION:-us-east-1}

echo "ğŸ“‹ Configuration:"
echo "   Cluster: $CLUSTER_NAME"
echo "   Region:  $AWS_REGION"

# Function to check if cluster exists
check_cluster_exists() {
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Check if cluster exists
if ! check_cluster_exists; then
    echo "âŒ Cluster $CLUSTER_NAME does not exist in region $AWS_REGION"
    exit 1
fi

echo "âœ… Cluster $CLUSTER_NAME exists"

# Update kubeconfig with retry
echo "ğŸ”§ Updating kubeconfig..."
for i in {1..5}; do
    if aws eks update-kubeconfig --region "$AWS_REGION" --name "$CLUSTER_NAME" --alias "$CLUSTER_NAME"; then
        echo "âœ… Kubeconfig updated successfully"
        break
    else
        echo "âš ï¸ Kubeconfig update attempt $i failed, retrying in 15 seconds..."
        sleep 15
        if [ $i -eq 5 ]; then
            echo "âŒ Failed to update kubeconfig after 5 attempts"
            exit 1
        fi
    fi
done

# Test cluster connectivity with detailed logging and limited retries
echo "ğŸ” Testing cluster connectivity..."
echo "ğŸ“‹ Starting connectivity tests with detailed logging..."

# Get detailed cluster information first
echo "ğŸ” Gathering cluster information..."
CLUSTER_INFO=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" 2>/dev/null)
if [ $? -eq 0 ]; then
    CLUSTER_STATUS=$(echo "$CLUSTER_INFO" | jq -r '.cluster.status // "UNKNOWN"')
    CLUSTER_ENDPOINT=$(echo "$CLUSTER_INFO" | jq -r '.cluster.endpoint // "UNKNOWN"')
    CLUSTER_VERSION=$(echo "$CLUSTER_INFO" | jq -r '.cluster.version // "UNKNOWN"')
    CLUSTER_PLATFORM_VERSION=$(echo "$CLUSTER_INFO" | jq -r '.cluster.platformVersion // "UNKNOWN"')
    
    echo "ğŸ“‹ Cluster Details:"
    echo "   Name: $CLUSTER_NAME"
    echo "   Status: $CLUSTER_STATUS"
    echo "   Endpoint: $CLUSTER_ENDPOINT"
    echo "   K8s Version: $CLUSTER_VERSION"
    echo "   Platform Version: $CLUSTER_PLATFORM_VERSION"
    
    if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
        echo "âŒ Cluster is not ACTIVE (current: $CLUSTER_STATUS)"
        echo "ï¿½ Wauit for cluster to be ACTIVE before proceeding"
        exit 1
    fi
else
    echo "âŒ Failed to describe cluster - check cluster name and permissions"
    exit 1
fi

# Check current AWS identity
echo "ğŸ” Checking AWS identity..."
AWS_IDENTITY=$(aws sts get-caller-identity 2>/dev/null)
if [ $? -eq 0 ]; then
    AWS_ACCOUNT=$(echo "$AWS_IDENTITY" | jq -r '.Account // "UNKNOWN"')
    AWS_USER_ARN=$(echo "$AWS_IDENTITY" | jq -r '.Arn // "UNKNOWN"')
    echo "ğŸ“‹ AWS Identity:"
    echo "   Account: $AWS_ACCOUNT"
    echo "   ARN: $AWS_USER_ARN"
else
    echo "âŒ Failed to get AWS identity - check credentials"
    exit 1
fi

# Test endpoint connectivity
echo "ğŸ” Testing cluster endpoint connectivity..."
if [ "$CLUSTER_ENDPOINT" != "UNKNOWN" ] && [ -n "$CLUSTER_ENDPOINT" ]; then
    echo "ğŸŒ Testing HTTPS connectivity to: $CLUSTER_ENDPOINT"
    
    # Test with curl
    CURL_OUTPUT=$(curl -k -s -w "HTTP_CODE:%{http_code};TIME:%{time_total}" --connect-timeout 15 --max-time 30 "$CLUSTER_ENDPOINT/healthz" 2>&1)
    CURL_EXIT_CODE=$?
    
    if [ $CURL_EXIT_CODE -eq 0 ]; then
        echo "âœ… Endpoint is reachable via HTTPS"
        echo "ğŸ“‹ Response: $CURL_OUTPUT"
    else
        echo "âš ï¸ Endpoint connectivity test failed"
        echo "ğŸ“‹ Error: $CURL_OUTPUT"
        echo "ğŸ’¡ This might indicate network or security group issues"
    fi
else
    echo "âŒ No valid cluster endpoint found"
    exit 1
fi

# Now test kubectl connectivity with limited retries
echo "ğŸ” Testing kubectl connectivity (max 3 attempts)..."
for i in {1..3}; do
    echo "ğŸ”„ Kubectl attempt $i/3..."
    
    # Show current kubectl context
    echo "ğŸ“‹ Current kubectl context:"
    kubectl config current-context 2>/dev/null || echo "   âŒ No current context set"
    
    # Show available contexts
    echo "ğŸ“‹ Available contexts:"
    kubectl config get-contexts 2>/dev/null || echo "   âŒ No contexts available"
    
    # Test kubectl with detailed output
    echo "ğŸ” Testing 'kubectl get nodes' with timeout..."
    KUBECTL_OUTPUT=$(timeout 30 kubectl get nodes -v=6 2>&1)
    KUBECTL_EXIT_CODE=$?
    
    if [ $KUBECTL_EXIT_CODE -eq 0 ]; then
        echo "âœ… Cluster is accessible via kubectl!"
        NODE_COUNT=$(echo "$KUBECTL_OUTPUT" | grep -v "^NAME" | grep -c "Ready\|NotReady" || echo "0")
        echo "ğŸ“Š Found $NODE_COUNT nodes:"
        echo "$KUBECTL_OUTPUT" | head -10
        break
    else
        echo "âŒ kubectl failed (exit code: $KUBECTL_EXIT_CODE)"
        echo "ğŸ“‹ kubectl output (last 10 lines):"
        echo "$KUBECTL_OUTPUT" | tail -10
        
        if [ $i -lt 3 ]; then
            echo "â³ Waiting 30 seconds before retry..."
            sleep 30
        else
            echo "âŒ Cluster is not accessible after 3 attempts"
            echo ""
            echo "ğŸ” FINAL DEBUGGING INFORMATION:"
            echo "================================"
            
            # Show cluster details again
            echo "ğŸ“‹ Cluster Status Check:"
            aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.{Status:status,Endpoint:endpoint,CreatedAt:createdAt}' --output table 2>/dev/null || echo "   âŒ Could not describe cluster"
            
            # Show kubectl config
            echo "ğŸ“‹ Kubectl Configuration:"
            kubectl config view --minify 2>/dev/null || echo "   âŒ Could not view kubectl config"
            
            # Show network test
            echo "ğŸ“‹ Network Connectivity:"
            if [ -n "$CLUSTER_ENDPOINT" ]; then
                echo "   Testing: $CLUSTER_ENDPOINT"
                nc -zv $(echo "$CLUSTER_ENDPOINT" | sed 's|https://||' | cut -d':' -f1) 443 2>&1 || echo "   âŒ Port 443 not reachable"
            fi
            
            echo ""
            echo "ğŸ’¡ TROUBLESHOOTING STEPS:"
            echo "========================"
            echo "1. Check if cluster is fully created in AWS Console"
            echo "2. Verify IAM permissions for EKS access"
            echo "3. Check security groups allow HTTPS (443) access"
            echo "4. Try accessing from AWS CloudShell:"
            echo "   aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME"
            echo "   kubectl get nodes"
            echo ""
            exit 1
        fi
    fi
done

# Get AWS account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "ğŸ“‹ AWS Account ID: $ACCOUNT_ID"

# Check if IAM policy exists, create if not
echo "ğŸ“‹ Checking IAM policy..."
POLICY_ARN="arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy"

if aws iam get-policy --policy-arn "$POLICY_ARN" >/dev/null 2>&1; then
    echo "âœ… IAM policy already exists: $POLICY_ARN"
else
    echo "ğŸ“‹ Creating IAM policy..."
    if ! curl -sS -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json; then
        echo "âŒ Failed to download IAM policy"
        exit 1
    fi
    
    if aws iam create-policy \
        --policy-name AWSLoadBalancerControllerIAMPolicy \
        --policy-document file://iam_policy.json; then
        echo "âœ… IAM policy created: $POLICY_ARN"
    else
        echo "âŒ Failed to create IAM policy"
        exit 1
    fi
fi

# Check if service account already exists
echo "ğŸ” Checking if service account exists..."
if kubectl get serviceaccount aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    echo "âœ… Service account already exists"
    
    # Check if it has the correct annotations
    SA_ROLE=$(kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || echo "")
    if [ -n "$SA_ROLE" ]; then
        echo "âœ… Service account has IAM role: $SA_ROLE"
    else
        echo "âš ï¸ Service account exists but has no IAM role annotation"
        echo "ğŸ”§ Recreating service account..."
        kubectl delete serviceaccount aws-load-balancer-controller -n kube-system || true
        sleep 5
    fi
fi

# Create service account if it doesn't exist or needs recreation
if ! kubectl get serviceaccount aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    echo "ğŸ”§ Creating IAM service account..."
    
    # Clean up any existing IAM role
    ROLE_NAME="AmazonEKSLoadBalancerControllerRole"
    if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
        echo "âš ï¸ IAM role $ROLE_NAME already exists, cleaning up..."
        eksctl delete iamserviceaccount \
            --cluster="$CLUSTER_NAME" \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --region="$AWS_REGION" || true
        sleep 10
    fi
    
    # Create new service account
    if eksctl create iamserviceaccount \
        --cluster="$CLUSTER_NAME" \
        --namespace=kube-system \
        --name=aws-load-balancer-controller \
        --role-name "$ROLE_NAME" \
        --attach-policy-arn="$POLICY_ARN" \
        --approve \
        --region="$AWS_REGION"; then
        echo "âœ… IAM service account created"
    else
        echo "âŒ Failed to create IAM service account"
        exit 1
    fi
fi

# Add EKS Helm repository
echo "ğŸ“¦ Adding EKS Helm repository..."
if ! helm repo add eks https://aws.github.io/eks-charts; then
    echo "âŒ Failed to add Helm repository"
    exit 1
fi

if ! helm repo update; then
    echo "âŒ Failed to update Helm repositories"
    exit 1
fi

# Check if Load Balancer Controller is already installed
echo "ğŸ” Checking if Load Balancer Controller is installed..."
if helm list -n kube-system | grep -q aws-load-balancer-controller; then
    echo "âœ… AWS Load Balancer Controller already installed, upgrading..."
    helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
        -n kube-system \
        --set clusterName="$CLUSTER_NAME" \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --set region="$AWS_REGION" \
        --wait --timeout=5m
else
    echo "ğŸš€ Installing AWS Load Balancer Controller..."
    helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
        -n kube-system \
        --set clusterName="$CLUSTER_NAME" \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --set region="$AWS_REGION" \
        --wait --timeout=5m
fi

# Verify installation
echo "âœ… Verifying installation..."
if kubectl get deployment aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    kubectl get deployment -n kube-system aws-load-balancer-controller
    kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
    echo "ğŸ‰ AWS Load Balancer Controller setup completed successfully!"
else
    echo "âŒ Load Balancer Controller deployment not found"
    exit 1
fi