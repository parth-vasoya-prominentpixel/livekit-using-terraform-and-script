#!/bin/bash

# Script to setup AWS Load Balancer Controller on EKS
# This script is idempotent - safe to run multiple times

echo "âš–ï¸ Setting up AWS Load Balancer Controller..."

# Check if CLUSTER_NAME is provided
if [ -z "$CLUSTER_NAME" ]; then
    echo "âŒ CLUSTER_NAME environment variable is required"
    echo "Usage: CLUSTER_NAME=your-cluster-name ./02-setup-load-balancer.sh"
    exit 1
fi

# Set AWS region (default to us-east-1 if not set)
AWS_REGION=${AWS_REGION:-us-east-1}

echo "ğŸ“‹ Configuration:"
echo "   Cluster: $CLUSTER_NAME"
echo "   Region:  $AWS_REGION"

# Function to check if cluster exists
check_cluster_exists() {
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Check if cluster exists
if ! check_cluster_exists; then
    echo "âŒ Cluster $CLUSTER_NAME does not exist in region $AWS_REGION"
    exit 1
fi

echo "âœ… Cluster $CLUSTER_NAME exists"

# Update kubeconfig with retry
echo "ğŸ”§ Updating kubeconfig..."
for i in {1..5}; do
    if aws eks update-kubeconfig --region "$AWS_REGION" --name "$CLUSTER_NAME" --alias "$CLUSTER_NAME"; then
        echo "âœ… Kubeconfig updated successfully"
        break
    else
        echo "âš ï¸ Kubeconfig update attempt $i failed, retrying in 15 seconds..."
        sleep 15
        if [ $i -eq 5 ]; then
            echo "âŒ Failed to update kubeconfig after 5 attempts"
            exit 1
        fi
    fi
done

# Test cluster connectivity with extended retry
echo "ğŸ” Testing cluster connectivity..."
for i in {1..10}; do
    if timeout 30 kubectl get nodes >/dev/null 2>&1; then
        echo "âœ… Cluster is accessible"
        kubectl get nodes --no-headers | wc -l | xargs echo "ğŸ“Š Found nodes:"
        break
    else
        echo "âš ï¸ Cluster not accessible, waiting 30 seconds... (attempt $i/10)"
        sleep 30
        if [ $i -eq 10 ]; then
            echo "âŒ Cluster is not accessible after 10 attempts"
            echo "ğŸ” Debugging information:"
            kubectl config current-context || echo "No current context"
            kubectl config get-contexts || echo "No contexts available"
            exit 1
        fi
    fi
done

# Get AWS account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "ğŸ“‹ AWS Account ID: $ACCOUNT_ID"

# Check if IAM policy exists, create if not
echo "ğŸ“‹ Checking IAM policy..."
POLICY_ARN="arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy"

if aws iam get-policy --policy-arn "$POLICY_ARN" >/dev/null 2>&1; then
    echo "âœ… IAM policy already exists: $POLICY_ARN"
else
    echo "ğŸ“‹ Creating IAM policy..."
    if ! curl -sS -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json; then
        echo "âŒ Failed to download IAM policy"
        exit 1
    fi
    
    if aws iam create-policy \
        --policy-name AWSLoadBalancerControllerIAMPolicy \
        --policy-document file://iam_policy.json; then
        echo "âœ… IAM policy created: $POLICY_ARN"
    else
        echo "âŒ Failed to create IAM policy"
        exit 1
    fi
fi

# Check if service account already exists
echo "ğŸ” Checking if service account exists..."
if kubectl get serviceaccount aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    echo "âœ… Service account already exists"
    
    # Check if it has the correct annotations
    SA_ROLE=$(kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || echo "")
    if [ -n "$SA_ROLE" ]; then
        echo "âœ… Service account has IAM role: $SA_ROLE"
    else
        echo "âš ï¸ Service account exists but has no IAM role annotation"
        echo "ğŸ”§ Recreating service account..."
        kubectl delete serviceaccount aws-load-balancer-controller -n kube-system || true
        sleep 5
    fi
fi

# Create service account if it doesn't exist or needs recreation
if ! kubectl get serviceaccount aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    echo "ğŸ”§ Creating IAM service account..."
    
    # Clean up any existing IAM role
    ROLE_NAME="AmazonEKSLoadBalancerControllerRole"
    if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
        echo "âš ï¸ IAM role $ROLE_NAME already exists, cleaning up..."
        eksctl delete iamserviceaccount \
            --cluster="$CLUSTER_NAME" \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --region="$AWS_REGION" || true
        sleep 10
    fi
    
    # Create new service account
    if eksctl create iamserviceaccount \
        --cluster="$CLUSTER_NAME" \
        --namespace=kube-system \
        --name=aws-load-balancer-controller \
        --role-name "$ROLE_NAME" \
        --attach-policy-arn="$POLICY_ARN" \
        --approve \
        --region="$AWS_REGION"; then
        echo "âœ… IAM service account created"
    else
        echo "âŒ Failed to create IAM service account"
        exit 1
    fi
fi

# Add EKS Helm repository
echo "ğŸ“¦ Adding EKS Helm repository..."
if ! helm repo add eks https://aws.github.io/eks-charts; then
    echo "âŒ Failed to add Helm repository"
    exit 1
fi

if ! helm repo update; then
    echo "âŒ Failed to update Helm repositories"
    exit 1
fi

# Check if Load Balancer Controller is already installed
echo "ğŸ” Checking if Load Balancer Controller is installed..."
if helm list -n kube-system | grep -q aws-load-balancer-controller; then
    echo "âœ… AWS Load Balancer Controller already installed, upgrading..."
    helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
        -n kube-system \
        --set clusterName="$CLUSTER_NAME" \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --set region="$AWS_REGION" \
        --wait --timeout=5m
else
    echo "ğŸš€ Installing AWS Load Balancer Controller..."
    helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
        -n kube-system \
        --set clusterName="$CLUSTER_NAME" \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --set region="$AWS_REGION" \
        --wait --timeout=5m
fi

# Verify installation
echo "âœ… Verifying installation..."
if kubectl get deployment aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
    kubectl get deployment -n kube-system aws-load-balancer-controller
    kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
    echo "ğŸ‰ AWS Load Balancer Controller setup completed successfully!"
else
    echo "âŒ Load Balancer Controller deployment not found"
    exit 1
fi